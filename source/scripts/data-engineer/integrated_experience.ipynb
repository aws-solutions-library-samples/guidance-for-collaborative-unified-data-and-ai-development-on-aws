{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c0e2af-b39f-400e-8d3e-88a8b4b315df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql project.redshift\n",
    "DROP TABLE IF EXISTS store_promotions;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26013c6f-42c5-427e-ae96-080576c4230d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql project.redshift\n",
    "DROP TABLE IF EXISTS store_sales;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b0976-a92d-4989-8b00-fb5aef344e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql project.redshift\n",
    "DROP TABLE IF EXISTS store_dim;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b62ff9-dc90-4eb9-9866-87dc2843d01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql project.redshift\n",
    "DROP TABLE IF EXISTS date_dim;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dfbff8-05d5-4685-8c6b-7a5a60876e87",
   "metadata": {},
   "source": [
    "### Create Date Dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de304860-289c-40bf-8eb7-e868ce3048cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql project.redshift\n",
    "create table date_dim(\n",
    "    date_key integer not null,\n",
    "    date_time timestamp,\n",
    "    week smallint not null,\n",
    "    month smallint not null,\n",
    "    quarter smallint not null,\n",
    "    year smallint not null,\n",
    "    weekday smallint not null,\n",
    "    primary key(date_key)\n",
    ")\n",
    "diststyle all;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d788459-3f2b-42ea-a1e7-f6d61de76abf",
   "metadata": {},
   "source": [
    "### Create Store Dimension table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bb5f6c-ae3a-495c-8fb4-6966811d9a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql project.redshift\n",
    "create table store_dim(\n",
    "    store_id integer not null,\n",
    "    store_name varchar(50) not null,\n",
    "    store_address varchar(100) not null,\n",
    "    city varchar(50) not null,\n",
    "    state varchar(10) not null,\n",
    "    country varchar(50) not null,\n",
    "    primary key(store_id)\n",
    ")\n",
    "diststyle all;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d60dfdc-6a44-4a53-9e77-ffccf89340c6",
   "metadata": {},
   "source": [
    "### Create Store Sales table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd388a4-bf4c-42d9-810d-bdf4b7c90ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql project.redshift\n",
    "create table store_sales(\n",
    "    store_id integer not null,\n",
    "    date_key integer not null,\n",
    "    total_sales decimal not null,\n",
    "    primary key(store_id, date_key),\n",
    "    foreign key(store_id) references store_dim(store_id),\n",
    "    foreign key(date_key) references date_dim(date_key)\n",
    ")\n",
    "distkey(store_id);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5b8ffc-0835-40b3-b926-35e52e12f996",
   "metadata": {},
   "source": [
    "### Create Store Promotions table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f941bc-9327-4014-8191-bdbdc7722988",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql project.redshift\n",
    "create table store_promotions(\n",
    "    store_id integer not null,\n",
    "    date_key integer not null,\n",
    "    promo smallint not null,\n",
    "    school_holiday smallint not null,\n",
    "    primary key(store_id, date_key),\n",
    "    foreign key(store_id) references store_dim(store_id),\n",
    "    foreign key(date_key) references date_dim(date_key)\n",
    ")\n",
    "distkey(store_id);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e7a3e6-5b72-40cd-8107-51583656c5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark\n",
    "%idle_timeout 2880\n",
    "%glue_version 4.0\n",
    "%worker_type G.1X\n",
    "%number_of_workers 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb80980f-aeb2-431d-aaa2-d3b6ba3aba27",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Import modules & Set the Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eae655-6fd5-4494-8b27-3fa97f76c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark\n",
    "import sys\n",
    "import boto3\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from pyspark.sql.functions import col, to_timestamp, date_format\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "  \n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83331660-0ed5-4c0f-8ec5-4cf84c2471a7",
   "metadata": {},
   "source": [
    "### Fetch the Default S3 Bucket and IAM Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53df70d-a3ab-4c5d-9c73-7952b6bc8c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "session = sagemaker.Session()\n",
    "s3_bucket = session.default_bucket()\n",
    "s3_prefix = sagemaker.Session().default_bucket_prefix\n",
    "\n",
    "print ('-----------------------------------------')\n",
    "print ('default bucket: ' + s3_bucket)\n",
    "print ('-----------------------------------------')\n",
    "\n",
    "iam_role = get_execution_role()\n",
    "print ('-----------------------------------------')\n",
    "print ('iam role: ' + iam_role)\n",
    "print ('-----------------------------------------')\n",
    "\n",
    "\n",
    "print ('-----------------------------------------')\n",
    "print ('s3_prefix: ' + s3_prefix)\n",
    "print ('-----------------------------------------')\n",
    "\n",
    "%send_to_remote --name project.spark  --local s3_bucket --remote s3_bucket\t\n",
    "%send_to_remote --name project.spark  --local iam_role --remote iam_role                                                                                              \n",
    "%send_to_remote --name project.spark  --local s3_prefix --remote s3_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a357d4e-943a-4704-ba8e-c070df6f7042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Walk through the local folder\n",
    "local_folder_path = '../../input_data/'\n",
    "bucket_name = s3_bucket\n",
    "s3_target_folder = s3_prefix\n",
    "\n",
    "for root, dirs, files in os.walk(local_folder_path):\n",
    "     for file_name in files:\n",
    "         local_path = os.path.join(root, file_name)  # Full local path\n",
    "         relative_path = os.path.relpath(local_path, local_folder_path)  # Relative to the base folder\n",
    "         s3_path = os.path.join(s3_target_folder, relative_path) if s3_target_folder else relative_path  # S3 path\n",
    "            \n",
    "         # Normalize S3 path for Unix-style paths\n",
    "         s3_path = s3_path.replace(\"\\\\\", \"/\")\n",
    "            \n",
    "         print(f\"Uploading {local_path} to s3://{bucket_name}/{s3_path}\")\n",
    "         s3_client.upload_file(local_path, bucket_name, s3_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f09689f-d86e-49ad-9221-fbf8c7190412",
   "metadata": {},
   "source": [
    "### Set up the variables\n",
    "\n",
    "#### Special notes for setting up the following variables:\n",
    "* s3_bucket => use the default bucket name printed in the above cell\n",
    "* iam_role => use the iam role arn printed in the above cell\n",
    "* redshift_url => Go to project > compute > select the redshift datawarehouse and copy the jdbc url. Since we are using IAM role to connect the database, use 'jdbc:redshift:iam://...' instead of 'jdbc:redshift://...'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2486a1-4d0d-49b6-9e8f-57395f6649dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark\n",
    "\n",
    "# Go to project > compute > select the redshift datawarehouse and copy the jdbc url. \n",
    "# Since we are using IAM role, use 'jdbc:redshift:iam://...' instead of 'jdbc:redshift://...'\n",
    "redshift_url = 'jdbc:redshift:iam://your_redshift_url:port/db'\n",
    "\n",
    "# Set the prefix for the redshift temp directory\n",
    "s3_redshift_prefix = s3_bucket + '/' + s3_prefix +'/redshift-temp/'\n",
    "\n",
    "s3_input_path = 's3://' + s3_bucket + '/' + s3_prefix + '/'\n",
    "date_dim_file = 'date_dim.csv'\n",
    "store_promotions_file = 'store_promotions.csv'\n",
    "store_sales_file = 'store_sales.csv'\n",
    "redshiftTmpDir = 's3://' + s3_bucket + '/' + s3_prefix\n",
    "date_dim_table = 'project.date_dim'\n",
    "store_dim_table = 'project.store_dim'\n",
    "store_sales_table = 'project.store_sales'\n",
    "store_promotions_table = 'project.store_promotions'\n",
    "glue_db = 'sales_db'\n",
    "rs_database = \"dev\"\n",
    "redshift_iam_role = iam_role"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d33e77a-dd26-40d4-bac7-1d8d6f00b980",
   "metadata": {},
   "source": [
    "### Read Date Dimension file from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06046a1-a904-4775-abd6-de5931ccb85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark\n",
    "date_dim_path = s3_input_path + date_dim_file\n",
    "# Script generated for node Amazon S3\n",
    "AmazonS3_date_dim_dyf = glueContext.create_dynamic_frame.from_options(\n",
    "    format_options={\n",
    "        \"quoteChar\": \"\\\"\", \n",
    "        \"withHeader\": True, \n",
    "        \"separator\": \",\"}, \n",
    "    connection_type=\"s3\", \n",
    "    format=\"csv\", \n",
    "    connection_options={\n",
    "        \"paths\": [date_dim_path], \n",
    "        \"recurse\": True},\n",
    "    transformation_ctx=\"AmazonS3_date_dim_dyf\")\n",
    "\n",
    "#AmazonS3_date_dim_dyf.toDF()\n",
    "\n",
    "AmazonS3_date_dim_df = AmazonS3_date_dim_dyf.toDF()\n",
    "# Show the first 10 rows\n",
    "print(\"First 10 rows of the DataFrame:\")\n",
    "AmazonS3_date_dim_df.show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdab8e01-6d88-471b-a098-b340483d7953",
   "metadata": {},
   "source": [
    "### Write Date Dimension in Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dbfe5d-514e-4af2-a4e7-cd2103bc0c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark\n",
    "redshift_url# Convert data types\n",
    "AmazonS3_date_dim_df = AmazonS3_date_dim_df.withColumn(\"date_key\", col(\"date_key\").cast(IntegerType()))\\\n",
    "                                                .withColumn(\"date_time\", to_timestamp(date_format(\"date_time\", \"yyyy-MM-dd HH:mm:ss\")))\\\n",
    "                                                .withColumn(\"week\", col(\"week\").cast(IntegerType()))\\\n",
    "                                                .withColumn(\"month\", col(\"month\").cast(IntegerType()))\\\n",
    "                                                .withColumn(\"quarter\", col(\"quarter\").cast(IntegerType()))\\\n",
    "                                                .withColumn(\"year\", col(\"year\").cast(IntegerType()))\\\n",
    "                                                .withColumn(\"weekday\", col(\"weekday\").cast(IntegerType()))\n",
    "\n",
    "# Write the date_dim in Redshift\n",
    "(\n",
    "    AmazonS3_date_dim_df.write.format(\"io.github.spark_redshift_community.spark.redshift\")\n",
    "    .option(\"url\", redshift_url)\n",
    "    .option(\"dbtable\", date_dim_table)\n",
    "    .option(\"tempdir\", redshiftTmpDir)\n",
    "    .option(\"aws_iam_role\", redshift_iam_role)\n",
    "    .mode(\"append\")\n",
    "    .save()\n",
    ")\n",
    "\n",
    "print(\"Data successfully uploaded to Redshift table: date_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaede328-6253-463c-82b8-c27541b27fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql project.redshift\n",
    "select * from date_dim limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab431024-13ca-45f1-ab6b-b263a4ba8c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql project.redshift\n",
    "select count(*) from date_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068f7ad0-6e81-473d-9416-515d2e62b1ee",
   "metadata": {},
   "source": [
    "### Read Store Sales file from S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ab2bac-7554-4ebe-a66e-f6c86d1c630e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%pyspark project.spark\n",
    "store_sales_path = s3_input_path + store_sales_file\n",
    "\n",
    "# Script generated for node Amazon S3\n",
    "AmazonS3_store_sales_dyf = glueContext.create_dynamic_frame.from_options(\n",
    "    format_options={\n",
    "        \"quoteChar\": \"\\\"\", \n",
    "        \"withHeader\": True, \n",
    "        \"separator\": \",\"}, \n",
    "    connection_type=\"s3\", \n",
    "    format=\"csv\", \n",
    "    connection_options={\n",
    "        \"paths\": [store_sales_path], \n",
    "        \"recurse\": True},\n",
    "    transformation_ctx=\"AmazonS3_store_sales_dyf\")\n",
    "\n",
    "AmazonS3_store_sales_df = AmazonS3_store_sales_dyf.toDF()\n",
    "# Show the first 10 rows\n",
    "print(\"First 10 rows of the DataFrame:\")\n",
    "AmazonS3_store_sales_df.show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893f2b25-f4c0-43be-8b48-79c7ba2ee0d7",
   "metadata": {},
   "source": [
    "### Extract Store Information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76c6a43-f574-4ed4-8058-8980ca0edbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark\n",
    "# Create a new DataFrame with the specified fields\n",
    "AmazonS3_store_dim_df = AmazonS3_store_sales_df.select(\n",
    "    \"store_id\",\n",
    "    \"store_name\",\n",
    "    \"store_address\",\n",
    "    \"city\",\n",
    "    \"state\",\n",
    "    \"country\"\n",
    ").distinct()\n",
    "\n",
    "# Show the first 10 rows\n",
    "print(\"First 10 rows of the DataFrame:\")\n",
    "AmazonS3_store_dim_df.show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546e6138-66f0-413f-8e52-b7c7ef8e01a1",
   "metadata": {},
   "source": [
    "### Write Store Dimension in Redshift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246e5b9c-9862-40af-a261-976dda4024bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark\n",
    "# Convert data types\n",
    "AmazonS3_store_dim_df = AmazonS3_store_dim_df.withColumn(\"store_id\", col(\"store_id\").cast(IntegerType()))\n",
    "\n",
    "\n",
    "# Write the store_dim in Redshift\n",
    "(\n",
    "    AmazonS3_store_dim_df.write.format(\"io.github.spark_redshift_community.spark.redshift\")\n",
    "    .option(\"url\", redshift_url)\n",
    "    .option(\"dbtable\", store_dim_table)\n",
    "    .option(\"tempdir\", redshiftTmpDir)\n",
    "    .option(\"aws_iam_role\", redshift_iam_role)\n",
    "    .mode(\"append\")\n",
    "    .save()\n",
    ")\n",
    "\n",
    "print(\"Data successfully uploaded to Redshift table: store_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901d919d-c923-46f6-a976-88213643bcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql project.redshift\n",
    "select * from store_dim limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bd07a0-cc90-44ac-8a2b-e841dc943ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql project.redshift\n",
    "select count(*) from store_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8505c2-675d-4956-83f9-5376206c9147",
   "metadata": {},
   "source": [
    "### Create Store Sales dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f0de1d-dde9-4bcd-8540-ec57b6058155",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark\n",
    "# Convert data types\n",
    "AmazonS3_store_sales_df = AmazonS3_store_sales_df.withColumn(\"store_id\", col(\"store_id\").cast(IntegerType()))\\\n",
    "                                                .withColumn(\"date_time\", to_timestamp(date_format(\"sales_date\", \"yyyy-MM-dd HH:mm:ss\")))\n",
    "\n",
    "# Join the DataFrames\n",
    "AmazonS3_store_sales_df_joined = AmazonS3_store_sales_df.join(\n",
    "    AmazonS3_date_dim_df,\n",
    "    AmazonS3_store_sales_df.date_time == AmazonS3_date_dim_df.date_time,\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "# Create the new DataFrame with the required columns\n",
    "AmazonS3_store_sales_df_updated = AmazonS3_store_sales_df_joined.select(\n",
    "    col(\"store_id\"),\n",
    "    col(\"date_key\"),\n",
    "    col(\"total_sales\")\n",
    ")\n",
    "\n",
    "# Show the first few rows of the new DataFrame\n",
    "AmazonS3_store_sales_df_updated.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbc2a86-180d-4f8f-8071-760c1b9522ef",
   "metadata": {},
   "source": [
    "### Write Store Sales in Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16778df-9ee3-400b-9769-21d0eaf962f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark\n",
    "# Write the store_sales in Redshift\n",
    "(\n",
    "    AmazonS3_store_sales_df_updated.write.format(\"io.github.spark_redshift_community.spark.redshift\")\n",
    "    .option(\"url\", redshift_url)\n",
    "    .option(\"dbtable\", store_sales_table)\n",
    "    .option(\"tempdir\", redshiftTmpDir)\n",
    "    .option(\"aws_iam_role\", redshift_iam_role)\n",
    "    .mode(\"append\")\n",
    "    .save()\n",
    ")\n",
    "\n",
    "print(\"Data successfully uploaded to Redshift table: store_sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295a3e40-a586-4ddf-b5f3-7164769d1980",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql project.redshift\n",
    "select * from store_sales limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f76b20e-f13f-4885-a453-50e2caa150e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql project.redshift\n",
    "select count(*) from store_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1462a14a-4264-4d02-ae0d-35e7edadaf5f",
   "metadata": {},
   "source": [
    "### Read Store Promotions file from S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0e6d5a-d7b8-44cd-9575-41c7f8c0e5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark\n",
    "store_promotions_path = s3_input_path + store_promotions_file\n",
    "\n",
    "# Script generated for node Amazon S3\n",
    "AmazonS3_store_promotions_dyf = glueContext.create_dynamic_frame.from_options(\n",
    "    format_options={\n",
    "        \"quoteChar\": \"\\\"\", \n",
    "        \"withHeader\": True, \n",
    "        \"separator\": \",\"}, \n",
    "    connection_type=\"s3\", \n",
    "    format=\"csv\", \n",
    "    connection_options={\n",
    "        \"paths\": [store_promotions_path], \n",
    "        \"recurse\": True},\n",
    "    transformation_ctx=\"AmazonS3_store_promotions_dyf\")\n",
    "\n",
    "AmazonS3_store_promotions_df = AmazonS3_store_promotions_dyf.toDF()\n",
    "# Show the first 10 rows\n",
    "print(\"First 10 rows of the DataFrame:\")\n",
    "AmazonS3_store_promotions_df.show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5338da-c73d-49df-926c-845eb705321a",
   "metadata": {},
   "source": [
    "### Create Store_Promotions updated dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8573b1bb-1f41-4694-8a8e-ef3cf88fd51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark\n",
    "# Convert data types\n",
    "AmazonS3_store_promotions_df = AmazonS3_store_promotions_df.withColumn(\"store_id\", col(\"store_id\").cast(IntegerType()))\\\n",
    "                                                .withColumn(\"date_time\", to_timestamp(date_format(\"sales_date\", \"yyyy-MM-dd HH:mm:ss\")))\\\n",
    "                                                .withColumn(\"promo\", col(\"promo\").cast(IntegerType()))\\\n",
    "                                                .withColumn(\"school_holiday\", col(\"school_holiday\").cast(IntegerType()))\n",
    "\n",
    "# Join the DataFrames\n",
    "AmazonS3_store_promotions_df_joined = AmazonS3_store_promotions_df.join(\n",
    "    AmazonS3_date_dim_df,\n",
    "    AmazonS3_store_promotions_df.date_time == AmazonS3_date_dim_df.date_time,\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "# Create the new DataFrame with the required columns\n",
    "AmazonS3_store_promotions_df_updated = AmazonS3_store_promotions_df_joined.select(\n",
    "    col(\"store_id\"),\n",
    "    col(\"date_key\"),\n",
    "    col(\"promo\"),\n",
    "    col(\"school_holiday\")\n",
    ")\n",
    "\n",
    "# Show the first few rows of the new DataFrame\n",
    "AmazonS3_store_promotions_df_updated.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77cd5a9-4ab6-4f69-813e-d3231d3eaf9b",
   "metadata": {},
   "source": [
    "### Write Store Promotions in Redshift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f595f44-718d-4ff0-897b-a709d512a24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark project.spark\n",
    "# Write the store_promotions in Redshift\n",
    "(\n",
    "    AmazonS3_store_promotions_df_updated.write.format(\"io.github.spark_redshift_community.spark.redshift\")\n",
    "    .option(\"url\", redshift_url)\n",
    "    .option(\"dbtable\", store_promotions_table)\n",
    "    .option(\"tempdir\", redshiftTmpDir)\n",
    "    .option(\"aws_iam_role\", redshift_iam_role)\n",
    "    .mode(\"append\")\n",
    "    .save()\n",
    ")\n",
    "\n",
    "print(\"Data successfully uploaded to Redshift table: store_promotions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2231cd7-f817-4649-9a82-047ee034567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql project.redshift\n",
    "select * from store_promotions limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13423244-c923-4602-8c5f-2bec9458080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql project.redshift\n",
    "select count(*) from store_promotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0146383-b144-4f6c-a616-7d0b10cd2902",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql project.redshift\n",
    "select  sm.store_id, sm.store_name, dm.date_time as sales_date, ss.total_sales, sp.promo,sp.school_holiday\n",
    "from    store_dim sm,\n",
    "        date_dim dm,\n",
    "        store_sales ss,\n",
    "        store_promotions sp\n",
    "where   sm.store_id = ss.store_id\n",
    "and     dm.date_key = ss.date_key\n",
    "and     sm.store_id = sp.store_id\n",
    "and     dm.date_key = sp.date_key\n",
    "order by sm.store_id\n",
    "limit 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
